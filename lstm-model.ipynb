{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short-Term Stock Price Prediction with LSTM\n",
    "\n",
    "This notebook implements a machine learning pipeline for predicting Indonesian stock prices (1-3 days ahead) using LSTM neural networks.\n",
    "\n",
    "## Features:\n",
    "- Multi-target prediction (1, 2, 3 days ahead)\n",
    "- Advanced feature engineering optimized for short-term prediction\n",
    "- Hyperparameter optimization\n",
    "- Comprehensive evaluation metrics\n",
    "- Caching system for efficiency\n",
    "\n",
    "## Target Stocks:\n",
    "- BBCA.JK (Bank Central Asia)\n",
    "- UNVR.JK (Unilever Corporation)\n",
    "- ITMG.JK (Indo Tambangraya Megah)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# TensorFlow/Keras imports\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, BatchNormalization, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "# Optimization imports\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target stocks\n",
    "STOCKS = [\n",
    "    'BBCA.JK',  # Bank Central Asia\n",
    "    'UNVR.JK',  # Unilever Corporation\n",
    "    'ITMG.JK',  # Indo Tambangraya Megah\n",
    "]\n",
    "\n",
    "STOCK_NAMES = {\n",
    "    'BBCA.JK': 'Bank Central Asia',\n",
    "    'UNVR.JK': 'Unilever Corporation',\n",
    "    'ITMG.JK': 'Indo Tambangraya Megah'\n",
    "}\n",
    "\n",
    "# Configuration optimized for short-term prediction\n",
    "SHORT_TERM_CONFIG = {\n",
    "    'sequence_length': 30,  # Shorter for short-term prediction\n",
    "    'prediction_days': [1, 2, 3],  # Predict 1, 2, 3 days ahead\n",
    "    'data_years': 20,  # 20 years of historical data\n",
    "    'validation_split': 0.15,  # Smaller for more training data\n",
    "    'test_split': 0.1\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Stocks: {STOCKS}\")\n",
    "print(f\"Prediction targets: {SHORT_TERM_CONFIG['prediction_days']} days\")\n",
    "print(f\"Sequence length: {SHORT_TERM_CONFIG['sequence_length']} days\")\n",
    "print(f\"Historical data: {SHORT_TERM_CONFIG['data_years']} years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cache Manager Class\n",
    "\n",
    "This class handles caching of data and models to improve efficiency and avoid re-downloading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheManager:\n",
    "    def __init__(self, cache_dir='cache'):\n",
    "        self.cache_dir = cache_dir\n",
    "        if not os.path.exists(cache_dir):\n",
    "            os.makedirs(cache_dir)\n",
    "\n",
    "    def get_stock_data_path(self, ticker, years):\n",
    "        return os.path.join(self.cache_dir, f\"{ticker}_{years}y_data.pkl\")\n",
    "\n",
    "    def get_model_path(self, model_name):\n",
    "        return os.path.join(self.cache_dir, f\"{model_name}.h5\")\n",
    "\n",
    "    def is_cache_valid(self, path, max_age_hours=24):\n",
    "        if not os.path.exists(path):\n",
    "            return False\n",
    "        file_time = datetime.fromtimestamp(os.path.getmtime(path))\n",
    "        now = datetime.now()\n",
    "        age = now - file_time\n",
    "        return age.total_seconds() < max_age_hours * 3600\n",
    "\n",
    "    def save_stock_data(self, ticker, years, data):\n",
    "        path = self.get_stock_data_path(ticker, years)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    def load_stock_data(self, ticker, years):\n",
    "        path = self.get_stock_data_path(ticker, years)\n",
    "        if self.is_cache_valid(path):\n",
    "            try:\n",
    "                with open(path, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading cached data: {e}\")\n",
    "        return None\n",
    "\n",
    "    def save_model(self, model, model_name):\n",
    "        path = self.get_model_path(model_name)\n",
    "        model.save(path)\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        path = self.get_model_path(model_name)\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                return load_model(path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading cached model: {e}\")\n",
    "        return None\n",
    "\n",
    "    def save_object(self, obj, name):\n",
    "        path = os.path.join(self.cache_dir, f\"{name}.pkl\")\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(obj, f)\n",
    "\n",
    "    def load_object(self, name):\n",
    "        path = os.path.join(self.cache_dir, f\"{name}.pkl\")\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                with open(path, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading cached object: {e}\")\n",
    "        return None\n",
    "\n",
    "# Initialize cache manager\n",
    "cache = CacheManager()\n",
    "print(\"Cache manager initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(ticker, years=20):\n",
    "    \"\"\"Retrieve historical stock data with caching optimization for short-term prediction\"\"\"\n",
    "    cached_data = cache.load_stock_data(ticker, years)\n",
    "    if cached_data is not None:\n",
    "        print(f\"Using cached data for {ticker}\")\n",
    "        return cached_data\n",
    "\n",
    "    print(f\"Downloading {ticker} data for {years} years from Yahoo Finance...\")\n",
    "\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=365 * years)\n",
    "\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        data = stock.history(start=start_date.strftime('%Y-%m-%d'),\n",
    "                            end=end_date.strftime('%Y-%m-%d'))\n",
    "\n",
    "        if data.empty:\n",
    "            print(f\"No data available for {ticker}\")\n",
    "            return None\n",
    "\n",
    "        data['Ticker'] = ticker\n",
    "        cache.save_stock_data(ticker, years, data)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading data for {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test data retrieval\n",
    "print(\"Testing data retrieval...\")\n",
    "sample_data = get_stock_data('BBCA.JK', 1)  # Test with 1 year\n",
    "if sample_data is not None:\n",
    "    print(f\"Sample data shape: {sample_data.shape}\")\n",
    "    print(f\"Date range: {sample_data.index.min()} to {sample_data.index.max()}\")\n",
    "    print(sample_data.head())\n",
    "else:\n",
    "    print(\"Failed to retrieve sample data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "This section adds technical indicators and features optimized for short-term prediction (1-3 days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_short_term_features(data):\n",
    "    \"\"\"\n",
    "    Add features optimized for short-term prediction (1-3 days)\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "\n",
    "    # === Short-term momentum features ===\n",
    "    # RSI with short periods\n",
    "    for period in [5, 9, 14]:\n",
    "        delta = df['Close'].diff()\n",
    "        gain = delta.where(delta > 0, 0)\n",
    "        loss = -delta.where(delta < 0, 0)\n",
    "        avg_gain = gain.rolling(window=period).mean()\n",
    "        avg_loss = loss.rolling(window=period).mean()\n",
    "        avg_loss = avg_loss.replace(0, 0.00001)\n",
    "        rs = avg_gain / avg_loss\n",
    "        df[f'RSI_{period}'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # === Moving Averages for short-term trends ===\n",
    "    for period in [3, 5, 7, 10, 15, 20]:\n",
    "        df[f'MA_{period}'] = df['Close'].rolling(window=period).mean()\n",
    "        df[f'EMA_{period}'] = df['Close'].ewm(span=period).mean()\n",
    "\n",
    "    # === MACD optimized for short-term ===\n",
    "    exp5 = df['Close'].ewm(span=5, adjust=False).mean()\n",
    "    exp13 = df['Close'].ewm(span=13, adjust=False).mean()\n",
    "    df['MACD_Short'] = exp5 - exp13\n",
    "    df['MACD_Short_Signal'] = df['MACD_Short'].ewm(span=5, adjust=False).mean()\n",
    "    df['MACD_Short_Hist'] = df['MACD_Short'] - df['MACD_Short_Signal']\n",
    "\n",
    "    # === Short-term volatility ===\n",
    "    for period in [3, 5, 7, 10]:\n",
    "        df[f'Volatility_{period}d'] = df['Close'].rolling(window=period).std()\n",
    "        df[f'Price_Change_{period}d'] = df['Close'].pct_change(period)\n",
    "\n",
    "    # === Bollinger Bands for short-term ===\n",
    "    for period in [10, 15, 20]:\n",
    "        df[f'BB_Middle_{period}'] = df['Close'].rolling(window=period).mean()\n",
    "        df[f'BB_Std_{period}'] = df['Close'].rolling(window=period).std()\n",
    "        df[f'BB_Upper_{period}'] = df[f'BB_Middle_{period}'] + (df[f'BB_Std_{period}'] * 2)\n",
    "        df[f'BB_Lower_{period}'] = df[f'BB_Middle_{period}'] - (df[f'BB_Std_{period}'] * 2)\n",
    "        df[f'BB_Position_{period}'] = (df['Close'] - df[f'BB_Lower_{period}']) / (df[f'BB_Upper_{period}'] - df[f'BB_Lower_{period}'])\n",
    "\n",
    "    # === Volume indicators ===\n",
    "    df['Volume_MA_5'] = df['Volume'].rolling(window=5).mean()\n",
    "    df['Volume_MA_10'] = df['Volume'].rolling(window=10).mean()\n",
    "    df['Volume_Ratio_5'] = df['Volume'] / df['Volume_MA_5']\n",
    "    df['Volume_Ratio_10'] = df['Volume'] / df['Volume_MA_10']\n",
    "\n",
    "    # === Price momentum ===\n",
    "    for period in [1, 2, 3, 5, 7]:\n",
    "        df[f'Price_Momentum_{period}d'] = df['Close'] / df['Close'].shift(period) - 1\n",
    "        df[f'High_Low_Ratio_{period}d'] = (df['High'] - df['Low']) / df['Close'].shift(period)\n",
    "\n",
    "    # === Intraday features ===\n",
    "    df['Daily_Return'] = df['Close'].pct_change()\n",
    "    df['High_Close_Ratio'] = df['High'] / df['Close']\n",
    "    df['Low_Close_Ratio'] = df['Low'] / df['Close']\n",
    "    df['Open_Close_Ratio'] = df['Open'] / df['Close']\n",
    "    df['Body_Size'] = abs(df['Close'] - df['Open']) / df['Close']\n",
    "    df['Upper_Shadow'] = (df['High'] - np.maximum(df['Close'], df['Open'])) / df['Close']\n",
    "    df['Lower_Shadow'] = (np.minimum(df['Close'], df['Open']) - df['Low']) / df['Close']\n",
    "\n",
    "    # === Lag features relevant for short-term ===\n",
    "    for lag in [1, 2, 3, 5, 7, 10]:\n",
    "        df[f'Close_Lag_{lag}'] = df['Close'].shift(lag)\n",
    "        df[f'Volume_Lag_{lag}'] = df['Volume'].shift(lag)\n",
    "        df[f'Return_Lag_{lag}'] = df['Daily_Return'].shift(lag)\n",
    "\n",
    "    # === Rolling statistics ===\n",
    "    for period in [5, 10, 15]:\n",
    "        df[f'Close_Rolling_Mean_{period}'] = df['Close'].rolling(window=period).mean()\n",
    "        df[f'Close_Rolling_Std_{period}'] = df['Close'].rolling(window=period).std()\n",
    "        df[f'Close_Rolling_Min_{period}'] = df['Close'].rolling(window=period).min()\n",
    "        df[f'Close_Rolling_Max_{period}'] = df['Close'].rolling(window=period).max()\n",
    "\n",
    "    # === Calendar effects ===\n",
    "    df['Day_of_Week'] = df.index.dayofweek\n",
    "    df['Month'] = df.index.month\n",
    "    df['Quarter'] = df.index.quarter\n",
    "    df['Is_Month_End'] = df.index.is_month_end.astype(int)\n",
    "    df['Is_Quarter_End'] = df.index.is_quarter_end.astype(int)\n",
    "\n",
    "    # Cyclic encoding for calendar features\n",
    "    df['Day_of_Week_sin'] = np.sin(2 * np.pi * df['Day_of_Week'] / 7)\n",
    "    df['Day_of_Week_cos'] = np.cos(2 * np.pi * df['Day_of_Week'] / 7)\n",
    "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "\n",
    "    # Drop NaN values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Test feature engineering\n",
    "if sample_data is not None:\n",
    "    print(\"Testing feature engineering...\")\n",
    "    enhanced_sample = add_short_term_features(sample_data)\n",
    "    print(f\"Original features: {sample_data.shape[1]}\")\n",
    "    print(f\"Enhanced features: {enhanced_sample.shape[1]}\")\n",
    "    print(f\"New feature count: {enhanced_sample.shape[1] - sample_data.shape[1]}\")\n",
    "    print(\"\\nSample of new features:\")\n",
    "    new_features = [col for col in enhanced_sample.columns if col not in sample_data.columns]\n",
    "    print(new_features[:10])  # Show first 10 new features\n",
    "else:\n",
    "    print(\"Cannot test feature engineering without sample data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_normalization(data, ticker):\n",
    "    \"\"\"\n",
    "    Optimized normalization for short-term prediction\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    scalers = {}\n",
    "\n",
    "    # Features using MinMaxScaler\n",
    "    minmax_features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "\n",
    "    # Features using StandardScaler (for normally distributed features)\n",
    "    standard_features = [col for col in df.columns if col not in minmax_features + ['Ticker']]\n",
    "\n",
    "    scalers[ticker] = {}\n",
    "\n",
    "    # MinMax scaling for price features\n",
    "    for feature in minmax_features:\n",
    "        if feature in df.columns:\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            values = df[feature].values.reshape(-1, 1)\n",
    "            scaler.fit(values)\n",
    "            scalers[ticker][feature] = scaler\n",
    "            df[feature] = scaler.transform(values).flatten()\n",
    "\n",
    "    # Standard scaling for technical features\n",
    "    for feature in standard_features:\n",
    "        if feature in df.columns and not df[feature].isna().any():\n",
    "            scaler = StandardScaler()\n",
    "            values = df[feature].values.reshape(-1, 1)\n",
    "            scaler.fit(values)\n",
    "            scalers[ticker][feature] = scaler\n",
    "            df[feature] = scaler.transform(values).flatten()\n",
    "\n",
    "    return df, scalers\n",
    "\n",
    "print(\"Normalization function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sequence Creation for Multi-Target Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_target_sequences(data, sequence_length=30, prediction_days=[1, 2, 3]):\n",
    "    \"\"\"\n",
    "    Create sequences for multi-target prediction (1, 2, 3 days)\n",
    "    \"\"\"\n",
    "    features = [col for col in data.columns if col != 'Ticker']\n",
    "    values = data[features].values\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(sequence_length, len(values) - max(prediction_days)):\n",
    "        X.append(values[i-sequence_length:i])\n",
    "\n",
    "        # Targets for 1, 2, 3 days ahead\n",
    "        y_targets = []\n",
    "        for days in prediction_days:\n",
    "            if i + days < len(values):\n",
    "                y_targets.append(values[i + days, features.index('Close')])\n",
    "            else:\n",
    "                y_targets.append(np.nan)\n",
    "\n",
    "        y.append(y_targets)\n",
    "\n",
    "    # Filter out rows with NaN targets\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    valid_indices = ~np.isnan(y).any(axis=1)\n",
    "    X = X[valid_indices]\n",
    "    y = y[valid_indices]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "print(\"Sequence creation function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Building Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_short_term_lstm_model(sequence_length, num_features, num_targets=3):\n",
    "    \"\"\"\n",
    "    LSTM model optimized for short-term prediction\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # First layer with more units to capture short-term patterns\n",
    "        LSTM(128, return_sequences=True, input_shape=(sequence_length, num_features)),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        # Second layer\n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        # Third layer\n",
    "        LSTM(32, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        # Dense layers for multi-target output\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_targets, name='predictions')  # Output for 1, 2, 3 days\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_optimized_short_term_model(sequence_length, num_features, num_targets, params):\n",
    "    \"\"\"\n",
    "    Model optimized with hyperparameter tuning\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # First LSTM layer\n",
    "    model.add(LSTM(\n",
    "        units=int(params['lstm_units_1']),\n",
    "        return_sequences=True,\n",
    "        input_shape=(sequence_length, num_features),\n",
    "        dropout=params['lstm_dropout_1'],\n",
    "        recurrent_dropout=params['recurrent_dropout_1']\n",
    "    ))\n",
    "\n",
    "    if params['use_batch_norm']:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    # Second LSTM layer\n",
    "    model.add(LSTM(\n",
    "        units=int(params['lstm_units_2']),\n",
    "        return_sequences=int(params['num_lstm_layers']) > 2,\n",
    "        dropout=params['lstm_dropout_2'],\n",
    "        recurrent_dropout=params['recurrent_dropout_2']\n",
    "    ))\n",
    "\n",
    "    if params['use_batch_norm']:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    # Third LSTM layer (optional)\n",
    "    if int(params['num_lstm_layers']) > 2:\n",
    "        model.add(LSTM(\n",
    "            units=int(params['lstm_units_3']),\n",
    "            return_sequences=False,\n",
    "            dropout=params['lstm_dropout_2'],\n",
    "            recurrent_dropout=params['recurrent_dropout_2']\n",
    "        ))\n",
    "\n",
    "        if params['use_batch_norm']:\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "    # Dense layers\n",
    "    model.add(Dense(\n",
    "        units=int(params['dense_units_1']),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(params['l2_reg'])\n",
    "    ))\n",
    "    model.add(Dropout(params['dense_dropout']))\n",
    "\n",
    "    model.add(Dense(\n",
    "        units=int(params['dense_units_2']),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(params['l2_reg'])\n",
    "    ))\n",
    "    model.add(Dropout(params['dense_dropout']))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(num_targets))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=params['learning_rate']),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "print(\"Model building functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_short_term_hyperparameters(X_train, y_train, X_val, y_val, num_targets=3):\n",
    "    \"\"\"\n",
    "    Hyperparameter optimization for short-term prediction model\n",
    "    \"\"\"\n",
    "    # Search space optimized for short-term prediction\n",
    "    dimensions = [\n",
    "        Integer(32, 256, name='lstm_units_1'),\n",
    "        Integer(16, 128, name='lstm_units_2'),\n",
    "        Integer(8, 64, name='lstm_units_3'),\n",
    "        Integer(16, 128, name='dense_units_1'),\n",
    "        Integer(8, 64, name='dense_units_2'),\n",
    "        Real(0.1, 0.4, name='lstm_dropout_1'),\n",
    "        Real(0.1, 0.4, name='lstm_dropout_2'),\n",
    "        Real(0.0, 0.3, name='recurrent_dropout_1'),\n",
    "        Real(0.0, 0.3, name='recurrent_dropout_2'),\n",
    "        Real(0.1, 0.4, name='dense_dropout'),\n",
    "        Real(0.0001, 0.01, prior='log-uniform', name='learning_rate'),\n",
    "        Real(0.0001, 0.01, prior='log-uniform', name='l2_reg'),\n",
    "        Integer(2, 3, name='num_lstm_layers'),\n",
    "        Categorical([True, False], name='use_batch_norm'),\n",
    "        Integer(16, 128, name='batch_size')\n",
    "    ]\n",
    "\n",
    "    default_params = {\n",
    "        'lstm_units_1': 128,\n",
    "        'lstm_units_2': 64,\n",
    "        'lstm_units_3': 32,\n",
    "        'dense_units_1': 64,\n",
    "        'dense_units_2': 32,\n",
    "        'lstm_dropout_1': 0.2,\n",
    "        'lstm_dropout_2': 0.2,\n",
    "        'recurrent_dropout_1': 0.1,\n",
    "        'recurrent_dropout_2': 0.1,\n",
    "        'dense_dropout': 0.2,\n",
    "        'learning_rate': 0.001,\n",
    "        'l2_reg': 0.001,\n",
    "        'num_lstm_layers': 2,\n",
    "        'use_batch_norm': True,\n",
    "        'batch_size': 32\n",
    "    }\n",
    "\n",
    "    @use_named_args(dimensions=dimensions)\n",
    "    def objective(**params):\n",
    "        try:\n",
    "            model = build_optimized_short_term_model(\n",
    "                X_train.shape[1],\n",
    "                X_train.shape[2],\n",
    "                num_targets,\n",
    "                params\n",
    "            )\n",
    "\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=100,\n",
    "                batch_size=int(params['batch_size']),\n",
    "                validation_data=(X_val, y_val),\n",
    "                callbacks=[early_stopping],\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            val_loss = np.min(history.history['val_loss'])\n",
    "            return val_loss\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in model evaluation: {e}\")\n",
    "            return 1e10\n",
    "\n",
    "    print(\"Starting hyperparameter optimization for short-term prediction...\")\n",
    "    result = gp_minimize(\n",
    "        func=objective,\n",
    "        dimensions=dimensions,\n",
    "        n_calls=50,\n",
    "        n_random_starts=10,\n",
    "        x0=[default_params[dim.name] for dim in dimensions],\n",
    "        random_state=42,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    best_params = {dim.name: value for dim, value in zip(dimensions, result.x)}\n",
    "\n",
    "    # Ensure integer parameters\n",
    "    for param in ['lstm_units_1', 'lstm_units_2', 'lstm_units_3', 'dense_units_1', 'dense_units_2', 'num_lstm_layers', 'batch_size']:\n",
    "        best_params[param] = int(best_params[param])\n",
    "\n",
    "    return best_params\n",
    "\n",
    "print(\"Hyperparameter optimization function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_short_term_model(model, X_test, y_test, scalers, ticker, prediction_days=[1, 2, 3]):\n",
    "    \"\"\"\n",
    "    Evaluate model for short-term prediction with separate metrics for each day\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Inverse transform predictions\n",
    "    close_scaler = scalers[ticker]['Close']\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for i, days in enumerate(prediction_days):\n",
    "        # Extract predictions for specific day\n",
    "        y_true_day = y_test[:, i]\n",
    "        y_pred_day = y_pred[:, i]\n",
    "\n",
    "        # Inverse transform\n",
    "        y_true_day_inv = close_scaler.inverse_transform(y_true_day.reshape(-1, 1)).flatten()\n",
    "        y_pred_day_inv = close_scaler.inverse_transform(y_pred_day.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_true_day_inv, y_pred_day_inv)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true_day_inv, y_pred_day_inv)\n",
    "        mape = mean_absolute_percentage_error(y_true_day_inv, y_pred_day_inv)\n",
    "        r2 = r2_score(y_true_day_inv, y_pred_day_inv)\n",
    "\n",
    "        # Direction accuracy\n",
    "        actual_direction = np.sign(np.diff(y_true_day_inv))\n",
    "        pred_direction = np.sign(np.diff(y_pred_day_inv))\n",
    "        direction_accuracy = np.mean(actual_direction == pred_direction) * 100\n",
    "\n",
    "        results[f'{days}d'] = {\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'MAPE': mape,\n",
    "            'R2': r2,\n",
    "            'Direction_Accuracy': direction_accuracy,\n",
    "            'y_true': y_true_day_inv,\n",
    "            'y_pred': y_pred_day_inv\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"Evaluation function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_short_term_predictions(results, ticker, prediction_days=[1, 2, 3]):\n",
    "    \"\"\"\n",
    "    Visualization of short-term prediction results\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, days in enumerate(prediction_days):\n",
    "        ax = axes[i]\n",
    "        day_results = results[f'{days}d']\n",
    "\n",
    "        y_true = day_results['y_true']\n",
    "        y_pred = day_results['y_pred']\n",
    "\n",
    "        # Plot actual vs predicted\n",
    "        ax.scatter(y_true, y_pred, alpha=0.5)\n",
    "        ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "        ax.set_xlabel('Actual Price')\n",
    "        ax.set_ylabel('Predicted Price')\n",
    "        ax.set_title(f'{ticker} - {days} Day Prediction\\nR² = {day_results[\"R2\"]:.4f}, Direction Acc = {day_results[\"Direction_Accuracy\"]:.1f}%')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot time series for last 100 predictions\n",
    "    ax = axes[3]\n",
    "    n_show = min(100, len(results['1d']['y_true']))\n",
    "    x_axis = range(n_show)\n",
    "\n",
    "    for days in prediction_days:\n",
    "        day_results = results[f'{days}d']\n",
    "        y_true = day_results['y_true'][-n_show:]\n",
    "        y_pred = day_results['y_pred'][-n_show:]\n",
    "\n",
    "        ax.plot(x_axis, y_pred, label=f'{days}d Pred', linestyle='--')\n",
    "\n",
    "    ax.plot(x_axis, results['1d']['y_true'][-n_show:], label='Actual', color='black', linewidth=2)\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Price')\n",
    "    ax.set_title('Time Series Comparison (Last 100 points)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualization function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Main Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_term_prediction_pipeline(stocks=STOCKS, optimize=True, train_new_model=True):\n",
    "    \"\"\"\n",
    "    Main pipeline for short-term prediction (1, 2, 3 days)\n",
    "    \"\"\"\n",
    "    config = SHORT_TERM_CONFIG\n",
    "    all_results = {}\n",
    "\n",
    "    for ticker in stocks:\n",
    "        print(f\"\\n===== Processing {ticker} ({STOCK_NAMES.get(ticker, ticker)}) =====\")\n",
    "\n",
    "        # 1. Get data\n",
    "        data = get_stock_data(ticker, config['data_years'])\n",
    "        if data is None or len(data) < 1000:\n",
    "            print(f\"Insufficient data for {ticker}\")\n",
    "            continue\n",
    "\n",
    "        # 2. Feature engineering\n",
    "        print(\"Adding short-term features...\")\n",
    "        enhanced_data = add_short_term_features(data)\n",
    "\n",
    "        if len(enhanced_data) < 500:\n",
    "            print(f\"Insufficient data after feature engineering for {ticker}\")\n",
    "            continue\n",
    "\n",
    "        # 3. Normalization\n",
    "        print(\"Normalizing data...\")\n",
    "        normalized_data, scalers = optimize_normalization(enhanced_data, ticker)\n",
    "\n",
    "        # 4. Create sequences\n",
    "        print(\"Creating sequences for multi-target prediction...\")\n",
    "        X, y = create_multi_target_sequences(\n",
    "            normalized_data.drop(columns=['Ticker']),\n",
    "            sequence_length=config['sequence_length'],\n",
    "            prediction_days=config['prediction_days']\n",
    "        )\n",
    "\n",
    "        if len(X) < 100:\n",
    "            print(f\"Insufficient sequences for {ticker}\")\n",
    "            continue\n",
    "\n",
    "        # 5. Split data\n",
    "        test_size = int(len(X) * config['test_split'])\n",
    "        val_size = int(len(X) * config['validation_split'])\n",
    "\n",
    "        X_train = X[:-test_size-val_size]\n",
    "        y_train = y[:-test_size-val_size]\n",
    "        X_val = X[-test_size-val_size:-test_size]\n",
    "        y_val = y[-test_size-val_size:-test_size]\n",
    "        X_test = X[-test_size:]\n",
    "        y_test = y[-test_size:]\n",
    "\n",
    "        print(f\"Data split - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "        # 6. Model training\n",
    "        model_name = f\"short_term_lstm_{ticker.replace('.', '_')}\"\n",
    "        model = None\n",
    "\n",
    "        if not train_new_model:\n",
    "            model = cache.load_model(model_name)\n",
    "\n",
    "        if model is None:\n",
    "            if optimize:\n",
    "                print(\"Optimizing hyperparameters...\")\n",
    "                params = optimize_short_term_hyperparameters(X_train, y_train, X_val, y_val)\n",
    "                print(f\"Best parameters: {params}\")\n",
    "\n",
    "                model = build_optimized_short_term_model(\n",
    "                    X_train.shape[1],\n",
    "                    X_train.shape[2],\n",
    "                    len(config['prediction_days']),\n",
    "                    params\n",
    "                )\n",
    "            else:\n",
    "                print(\"Using default model...\")\n",
    "                model = build_short_term_lstm_model(\n",
    "                    X_train.shape[1],\n",
    "                    X_train.shape[2],\n",
    "                    len(config['prediction_days'])\n",
    "                )\n",
    "\n",
    "            # Train model\n",
    "            callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "            ]\n",
    "\n",
    "            print(\"Training model...\")\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=150,\n",
    "                batch_size=32,\n",
    "                validation_data=(X_val, y_val),\n",
    "                callbacks=callbacks,\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            # Save model\n",
    "            cache.save_model(model, model_name)\n",
    "\n",
    "            # Plot training history\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(history.history['loss'], label='Train Loss')\n",
    "            plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "            plt.title(f'{ticker} - Training Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(history.history['mae'], label='Train MAE')\n",
    "            plt.plot(history.history['val_mae'], label='Val MAE')\n",
    "            plt.title(f'{ticker} - Training MAE')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('MAE')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # 7. Evaluation\n",
    "        print(\"Evaluating model...\")\n",
    "        results = evaluate_short_term_model(\n",
    "            model, X_test, y_test, scalers, ticker, config['prediction_days']\n",
    "        )\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\n{ticker} ({STOCK_NAMES.get(ticker, ticker)}) Results:\")\n",
    "        print(\"-\" * 60)\n",
    "        for days in config['prediction_days']:\n",
    "            day_results = results[f'{days}d']\n",
    "            print(f\"{days} Day Prediction:\")\n",
    "            print(f\"  R²: {day_results['R2']:.4f}\")\n",
    "            print(f\"  RMSE: {day_results['RMSE']:.4f}\")\n",
    "            print(f\"  MAE: {day_results['MAE']:.4f}\")\n",
    "            print(f\"  MAPE: {day_results['MAPE']:.2f}%\")\n",
    "            print(f\"  Direction Accuracy: {day_results['Direction_Accuracy']:.1f}%\")\n",
    "\n",
    "        # 8. Visualization\n",
    "        plot_short_term_predictions(results, ticker, config['prediction_days'])\n",
    "\n",
    "        # 9. Store results\n",
    "        all_results[ticker] = {\n",
    "            'model': model,\n",
    "            'results': results,\n",
    "            'scalers': scalers,\n",
    "            'config': config\n",
    "        }\n",
    "\n",
    "    return all_results\n",
    "\n",
    "print(\"Main pipeline function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Execute the Pipeline\n",
    "\n",
    "Now let's run the complete pipeline. You can adjust the parameters below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for execution\n",
    "stocks_to_process = STOCKS  # Can be changed to process specific stocks\n",
    "enable_optimization = True   # Set to False for faster execution without hyperparameter tuning\n",
    "train_new_models = True     # Set to False to use cached models if available\n",
    "\n",
    "print(\"Starting Short-Term Stock Prediction Pipeline\")\n",
    "print(f\"Stocks: {stocks_to_process}\")\n",
    "print(f\"Prediction targets: {SHORT_TERM_CONFIG['prediction_days']} days\")\n",
    "print(f\"Historical data: {SHORT_TERM_CONFIG['data_years']} years\")\n",
    "print(f\"Sequence length: {SHORT_TERM_CONFIG['sequence_length']} days\")\n",
    "print(f\"Hyperparameter optimization: {enable_optimization}\")\n",
    "print(f\"Train new models: {train_new_models}\")\n",
    "\n",
    "# Run the pipeline\n",
    "results = short_term_prediction_pipeline(\n",
    "    stocks=stocks_to_process,\n",
    "    optimize=enable_optimization,\n",
    "    train_new_model=train_new_models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for ticker, result in results.items():\n",
    "    print(f\"\\n{ticker} ({STOCK_NAMES.get(ticker, ticker)}):\")\n",
    "    for days in SHORT_TERM_CONFIG['prediction_days']:\n",
    "        day_results = result['results'][f'{days}d']\n",
    "        print(f\"  {days}d: R²={day_results['R2']:.4f}, RMSE={day_results['RMSE']:.4f}, Dir.Acc={day_results['Direction_Accuracy']:.1f}%\")\n",
    "\n",
    "print(\"\\nPipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Additional Analysis (Optional)\n",
    "\n",
    "You can add more analysis cells here for deeper insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame for easy comparison\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = []\n",
    "for ticker, result in results.items():\n",
    "    for days in SHORT_TERM_CONFIG['prediction_days']:\n",
    "        day_results = result['results'][f'{days}d']\n",
    "        summary_data.append({\n",
    "            'Stock': ticker,\n",
    "            'Company': STOCK_NAMES.get(ticker, ticker),\n",
    "            'Prediction_Days': days,\n",
    "            'R2': day_results['R2'],\n",
    "            'RMSE': day_results['RMSE'],\n",
    "            'MAE': day_results['MAE'],\n",
    "            'MAPE': day_results['MAPE'],\n",
    "            'Direction_Accuracy': day_results['Direction_Accuracy']\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nDetailed Results Summary:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save results to CSV\n",
    "summary_df.to_csv('stock_prediction_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'stock_prediction_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparative performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# R² comparison\n",
    "axes[0,0].bar(range(len(summary_df)), summary_df['R2'])\n",
    "axes[0,0].set_title('R² Score Comparison')\n",
    "axes[0,0].set_ylabel('R² Score')\n",
    "axes[0,0].set_xticks(range(len(summary_df)))\n",
    "axes[0,0].set_xticklabels([f\"{row['Stock']}\\n{row['Prediction_Days']}d\" for _, row in summary_df.iterrows()], rotation=45)\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0,1].bar(range(len(summary_df)), summary_df['RMSE'])\n",
    "axes[0,1].set_title('RMSE Comparison')\n",
    "axes[0,1].set_ylabel('RMSE')\n",
    "axes[0,1].set_xticks(range(len(summary_df)))\n",
    "axes[0,1].set_xticklabels([f\"{row['Stock']}\\n{row['Prediction_Days']}d\" for _, row in summary_df.iterrows()], rotation=45)\n",
    "\n",
    "# Direction Accuracy comparison\n",
    "axes[1,0].bar(range(len(summary_df)), summary_df['Direction_Accuracy'])\n",
    "axes[1,0].set_title('Direction Accuracy Comparison')\n",
    "axes[1,0].set_ylabel('Direction Accuracy (%)')\n",
    "axes[1,0].set_xticks(range(len(summary_df)))\n",
    "axes[1,0].set_xticklabels([f\"{row['Stock']}\\n{row['Prediction_Days']}d\" for _, row in summary_df.iterrows()], rotation=45)\n",
    "\n",
    "# MAPE comparison\n",
    "axes[1,1].bar(range(len(summary_df)), summary_df['MAPE'])\n",
    "axes[1,1].set_title('MAPE Comparison')\n",
    "axes[1,1].set_ylabel('MAPE (%)')\n",
    "axes[1,1].set_xticks(range(len(summary_df)))\n",
    "axes[1,1].set_xticklabels([f\"{row['Stock']}\\n{row['Prediction_Days']}d\" for _, row in summary_df.iterrows()], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Model Saving and Loading for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results for future use\n",
    "cache.save_object(results, 'final_results')\n",
    "print(\"All results saved to cache!\")\n",
    "\n",
    "# Example of how to load and use a specific model for new predictions\n",
    "def make_prediction(ticker, new_data, prediction_days=[1, 2, 3]):\n",
    "    \"\"\"\n",
    "    Make new predictions using trained model\n",
    "    \"\"\"\n",
    "    if ticker not in results:\n",
    "        print(f\"No trained model available for {ticker}\")\n",
    "        return None\n",
    "    \n",
    "    model = results[ticker]['model']\n",
    "    scalers = results[ticker]['scalers']\n",
    "    config = results[ticker]['config']\n",
    "    \n",
    "    # Process new data (feature engineering + normalization)\n",
    "    enhanced_data = add_short_term_features(new_data)\n",
    "    \n",
    "    # Apply same normalization\n",
    "    normalized_data = enhanced_data.copy()\n",
    "    for feature, scaler in scalers[ticker].items():\n",
    "        if feature in normalized_data.columns:\n",
    "            values = normalized_data[feature].values.reshape(-1, 1)\n",
    "            normalized_data[feature] = scaler.transform(values).flatten()\n",
    "    \n",
    "    # Create sequence for prediction\n",
    "    features = [col for col in normalized_data.columns if col != 'Ticker']\n",
    "    values = normalized_data[features].values\n",
    "    \n",
    "    if len(values) < config['sequence_length']:\n",
    "        print(f\"Need at least {config['sequence_length']} days of data\")\n",
    "        return None\n",
    "    \n",
    "    # Use last sequence for prediction\n",
    "    X = values[-config['sequence_length']:].reshape(1, config['sequence_length'], -1)\n",
    "    \n",
    "    # Make prediction\n",
    "    pred_normalized = model.predict(X)\n",
    "    \n",
    "    # Inverse transform\n",
    "    close_scaler = scalers[ticker]['Close']\n",
    "    predictions = close_scaler.inverse_transform(pred_normalized).flatten()\n",
    "    \n",
    "    # Return predictions with labels\n",
    "    pred_dict = {}\n",
    "    for i, days in enumerate(prediction_days):\n",
    "        pred_dict[f'{days}d'] = predictions[i]\n",
    "    \n",
    "    return pred_dict\n",
    "\n",
    "print(\"Prediction function defined! Use make_prediction(ticker, new_data) to get predictions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}